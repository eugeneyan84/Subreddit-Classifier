{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubReddit Web Scraping, Analysis & Classification: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Taking the role of a business analyst in a company specializing in home appliances (light fixtures, vacuum cleaners, etc), I am tasked to perform research on smart devices as we explore the option of expanding and enhancing our current line of products. We have acquired a very large market-research dataset from varied sources to aid the exploration effort, unfortunately part of the metadata on data origin was lost. All that we know is the data segment that was collected from Reddit originated from r/homeassistant and r/homeautomation. Owing to time and data-storage constraints, we only have time to narrow down on posts that originate from one subreddit that is more relevant to our needs, which is to explore the current trends and issues of popular, easy to install, smart-home devices. Data from the less relevant subreddit is expected to be disposed so as to alleviate the data-storage issue.\n",
    "\n",
    "<b>The goal would be to build a text classifier based on a small subset of subreddit data scraped from the Web, that can differentiate between posts from r/homeassistant and those from r/homeautomation, so that we can reliably classify and preserve the relevant text data from our market-research dataset.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "This project is focused on building a text classifier to differentiate between posts from r/homeassistant and r/homeautomation.\n",
    "\n",
    "*Data Collection* phase began with scraping of the Reddit JSON API and it yielded 501 posts from each subreddit. \n",
    "\n",
    "*Data Cleaning* phase revealed some r/homeassistant posts containing programmatic code, which could impact text analysis efforts. Both subreddits had a proportion of their posts containing only titles, as users did not see any further need to elaborate on their posts within the main body.\n",
    "\n",
    "In the *Data Analysis* phase, distributions for word counts of titles and posts where charted out, but both subreddits did not show any significant differences in terms of content length. During wordcloud and top 20 frequently-used words analysis, it indicated that users in r/homeassistant were centered around tinkering with code and individual components (e.g. single-board computers, wireless modules, sensors, etc) to create new or enhance existing devices. On the otherhand, r/homeautmation posts tended towards smart-home devices that are well known consumer brands on the market, without having to go to the level of crafting a smart-home device together from separate parts. All in all, r/homeassistant seem to be attracting hobby electronic enthusiasts, while r/homeautomation is a more suitable place for typical consumers of tech gadgets.\n",
    "\n",
    "In *Pre-processing, Model Creation & Benchmarking* phase, Term Frequency-Inverse Document Frequency vectorizer was used to tranform our text data into vectors. r/homeautomation was set as the positive class as it fulfilled the business requirement of our company wanting to explore issues faced by owners/prospective buyers of smart-home devices. Scoring metrics were determined to be test-set accuracy score and recall score.\n",
    "\n",
    "Multinomial Naive Bayes classifier and Support Vector classifier were modeled, fitted and scored, eventually showing that the Multinomial Naive Bayes classifier performed slightly better in terms of test-set accuracy score. Subsequent efforts to tune the Multinomial Naive Bayes classifier and Support Vector Classifier did not yield significantly better results, especially in terms of test-set accuracy score and recall score.\n",
    "\n",
    "In favor of the simpler model that was faster to train at the same time, we selected the Multinomial Naive Bayes classifier (alpha=1.0) as our choice model for classification of incoming content from the 2 subreddits, with the aim of identifying r/homeautomation posts to further our business objectives. It has a test-set Accuracy Score of 81.6% and Recall Score of 0.824."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:23:07.175388Z",
     "start_time": "2020-11-28T06:23:07.170509Z"
    }
   },
   "source": [
    "# Data Collection\n",
    "\n",
    "The purpose of this section would be to collect posts from the above chosen subreddits, which would form our dataset for purpose of analysis and subsequent classification.\n",
    "\n",
    "We would be using the [Reddit JSON API](https://github.com/reddit-archive/reddit/wiki/JSON) for retrieving of posts in JSON format. From the original web-address `https://www.reddit.com/r/homeassistant`, we will transform it into `https://www.reddit.com/r/homeassistant.json` which immediately grants us access to the JSON API of the Home Assistant subreddit.\n",
    "\n",
    "The key-value pair structure of the response would make it intuitive to navigate and manipulate the response data like a conventional Python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:23:59.804803Z",
     "start_time": "2020-11-28T06:23:59.800912Z"
    }
   },
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:39.768735Z",
     "start_time": "2020-12-06T12:12:39.137736Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define user-agent for HTTP request header\n",
    "\n",
    "Adhering to the [guidelines](https://github.com/reddit-archive/reddit/wiki/API) laid down by Reddit on web scraping, we define a user-agent value for usage when making HTTP requests to the Reddit API for scraping data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:39.774593Z",
     "start_time": "2020-12-06T12:12:39.769711Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate user-agent\n",
    "user_agent = 'android:com.hwrobotics.sc:v1.3.0'\n",
    "\n",
    "h_assistant_url = 'https://www.reddit.com/r/homeassistant.json'\n",
    "\n",
    "h_automation_url = 'https://www.reddit.com/r/homeautomation.json'\n",
    "\n",
    "subreddit_url_list = [h_assistant_url, h_automation_url]\n",
    "subreddit_title_list = ['home_assistant', 'home_automation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a single HTTP request and inspect the response structure:\n",
    "\n",
    "We execute a HTTP GET requests to the Home Improvement JSON API, whilst supplying the user agent value that has been defined in the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:40.878879Z",
     "start_time": "2020-12-06T12:12:39.776544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP response: 200 OK\n"
     ]
    }
   ],
   "source": [
    "# execute a GET request\n",
    "res = requests.get('https://www.reddit.com/r/homeautomation.json', headers={'User-agent': user_agent})\n",
    "\n",
    "# check that response is successful\n",
    "print('HTTP response: {} {}'.format(res.status_code, res.reason))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T10:25:07.866408Z",
     "start_time": "2020-11-28T10:25:07.860524Z"
    }
   },
   "source": [
    "After verifying that the HTTP requests has yielded a successful response, we begin to inspect the HTTP response data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:40.891565Z",
     "start_time": "2020-12-06T12:12:40.879854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'data'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .json() function returns a JSON object of the result, which resembles a Python dictionary\n",
    "reddit_dict = res.json()\n",
    "reddit_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T10:26:54.360921Z",
     "start_time": "2020-11-28T10:26:54.356042Z"
    }
   },
   "source": [
    "Among the 2 keys, we navigate to the `data` node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:40.939390Z",
     "start_time": "2020-12-06T12:12:40.897422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['modhash', 'dist', 'children', 'after', 'before'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_node = reddit_dict['data']\n",
    "data_node.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T11:34:43.361919Z",
     "start_time": "2020-11-28T11:34:43.354112Z"
    }
   },
   "source": [
    "The data segment should have a list of posts, so we navigate further to the `children` node within it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:40.952077Z",
     "start_time": "2020-12-06T12:12:40.940366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = reddit_dict['data']['children']\n",
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above length check, it indicates that we have found the 27 posts that are sent as a response to the initial HTTP GET request. We inspect the `data` node of one of the posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.068221Z",
     "start_time": "2020-12-06T12:12:40.954030Z"
    }
   },
   "outputs": [],
   "source": [
    "single_post = reddit_dict['data']['children'][5]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.081885Z",
     "start_time": "2020-12-06T12:12:41.071150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'gildings', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'all_awardings', 'awarders', 'media_only', 'link_flair_template_id', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_post.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.123854Z",
     "start_time": "2020-12-06T12:12:41.083838Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stickied</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_video</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>1.607235e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>Multiple August Smart Lock Pros with Connect</td>\n",
       "      <td>I have three August Smart Lock Pros with conne...</td>\n",
       "      <td>https://www.reddit.com/r/homeautomation/commen...</td>\n",
       "      <td>r/homeautomation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stickied   created_utc  is_video  \\\n",
       "0     False  1.607235e+09     False   \n",
       "\n",
       "                                          title  \\\n",
       "0  Multiple August Smart Lock Pros with Connect   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I have three August Smart Lock Pros with conne...   \n",
       "\n",
       "                                                 url subreddit_name_prefixed  \n",
       "0  https://www.reddit.com/r/homeautomation/commen...        r/homeautomation  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([single_post]) \\\n",
    "    .loc[:,['stickied','created_utc', 'is_video', 'title', 'selftext','url','subreddit_name_prefixed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.201934Z",
     "start_time": "2020-12-06T12:12:41.124830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/r/homeautomation/comments/k7oen6/multiple_august_smart_lock_pros_with_connect/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pd.DataFrame([single_post]).loc[:,'url'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each post would contain a lot of metadata, but the fields we would be interested in are the `selftext` and the `title`. These 2 fields hold the title and main contents of the post:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create web scraping procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters for iterative web scraping\n",
    "\n",
    "Each HTTP GET request would retrieve 25 posts (1st request likely retrieve more than 25 posts, if the subreddit contains at least 1 stickied post). We aim to retrieve 500 posts per subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.211693Z",
     "start_time": "2020-12-06T12:12:41.202909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run with is_live_data_collection=True only for live data collection. \n",
    "#\n",
    "# For demo purpose, set is_live_data_collection=False to scrape only 50 posts for proof-of-concept.\n",
    "#\n",
    "is_live_data_collection = False\n",
    "\n",
    "if is_live_data_collection:\n",
    "    target_number_of_post = 500\n",
    "else:\n",
    "    target_number_of_post = 50\n",
    "\n",
    "post_count_per_request = 25\n",
    "\n",
    "iteration_count = int(target_number_of_post / post_count_per_request)\n",
    "iteration_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define web scraping function for 1 subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.306364Z",
     "start_time": "2020-12-06T12:12:41.213646Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_reddit_posts(url, iter_count, after_value=None):\n",
    "    posts = []\n",
    "    after = after_value\n",
    "\n",
    "    for index in range(iter_count):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        req = requests.get(current_url, headers={'User-agent': user_agent})\n",
    "\n",
    "        if req.status_code != 200:\n",
    "            print('[collect_reddit_posts] Status error: {} {}', req.status_code, req.reason)\n",
    "            break\n",
    "        else:\n",
    "            print('[collect_reddit_posts] Iteration #{}, HTTP response: {} {}'.format(index, req.status_code, req.reason))\n",
    "\n",
    "        request_dict = req.json()\n",
    "        subreddit_posts = [node['data'] for node in request_dict['data']['children']]\n",
    "        posts.extend(subreddit_posts)\n",
    "        after = request_dict['data']['after']\n",
    "        \n",
    "        sleep_duration = random.randint(10, 60) # set delay of random no. of seconds\n",
    "        print('[collect_reddit_posts] Extracted {} post(s), now sleeping for {}s'.format(len(subreddit_posts), sleep_duration))\n",
    "        time.sleep(sleep_duration) # execute delay before next iteration\n",
    "        \n",
    "    return posts, after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define aggregation function to collectively scrap and save data from multiple subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.321981Z",
     "start_time": "2020-12-06T12:12:41.308318Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_reddit_posts(url_list, filename_list, iter_count, after_list):\n",
    "    print('url list: {}'.format(url_list))\n",
    "    print('filename list: {}'.format(filename_list))\n",
    "    print('iter_count: {}'.format(iter_count))\n",
    "    for item in zip(url_list, filename_list, after_list):\n",
    "        print('[aggregate_reddit_posts] Scraping post for [{}]'.format(item[0]))\n",
    "        posts, after = collect_reddit_posts(\n",
    "                                        item[0], \n",
    "                                        iter_count, \n",
    "                                        item[2])\n",
    "        print('[aggregate_reddit_posts] Saving posts to csv file: {}_[{}].csv'.format(item[1], after))\n",
    "        pd.DataFrame(posts).to_csv('../data/{}_[{}].csv'.format(item[1], after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform web scraping and saving of data from the 2 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.430316Z",
     "start_time": "2020-12-06T12:12:41.323934Z"
    }
   },
   "outputs": [],
   "source": [
    "after_list = [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:12:41.444957Z",
     "start_time": "2020-12-06T12:12:41.431293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.reddit.com/r/homeassistant.json',\n",
       " 'https://www.reddit.com/r/homeautomation.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:15:01.424254Z",
     "start_time": "2020-12-06T12:12:41.447885Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url list: ['https://www.reddit.com/r/homeassistant.json', 'https://www.reddit.com/r/homeautomation.json']\n",
      "filename list: ['home_assistant', 'home_automation']\n",
      "iter_count: 2\n",
      "[aggregate_reddit_posts] Scraping post for [https://www.reddit.com/r/homeassistant.json]\n",
      "https://www.reddit.com/r/homeassistant.json\n",
      "[collect_reddit_posts] Iteration #0, HTTP response: 200 OK\n",
      "[collect_reddit_posts] Extracted 26 post(s), now sleeping for 34s\n",
      "https://www.reddit.com/r/homeassistant.json?after=t3_k7k3m4\n",
      "[collect_reddit_posts] Iteration #1, HTTP response: 200 OK\n",
      "[collect_reddit_posts] Extracted 25 post(s), now sleeping for 24s\n",
      "[aggregate_reddit_posts] Saving posts to csv file: home_assistant_[t3_k796m5].csv\n",
      "[aggregate_reddit_posts] Scraping post for [https://www.reddit.com/r/homeautomation.json]\n",
      "https://www.reddit.com/r/homeautomation.json\n",
      "[collect_reddit_posts] Iteration #0, HTTP response: 200 OK\n",
      "[collect_reddit_posts] Extracted 25 post(s), now sleeping for 40s\n",
      "https://www.reddit.com/r/homeautomation.json?after=t3_k7b7xn\n",
      "[collect_reddit_posts] Iteration #1, HTTP response: 200 OK\n",
      "[collect_reddit_posts] Extracted 25 post(s), now sleeping for 38s\n",
      "[aggregate_reddit_posts] Saving posts to csv file: home_automation_[t3_k6yerb].csv\n"
     ]
    }
   ],
   "source": [
    "aggregate_reddit_posts(subreddit_url_list, subreddit_title_list, iteration_count, after_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subreddit data in the saved CSV files can be used for cleaning and analysis in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "134px",
    "width": "445px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
